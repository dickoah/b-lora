{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda info --envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from blora_utils import BLOCKS, filter_lora, scale_lora\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from diffusers import (\n",
    "    ControlNetModel, \n",
    "    DDIMScheduler,\n",
    "    AutoencoderKL, \n",
    "    AutoPipelineForText2Image, \n",
    "    AutoPipelineForImage2Image,\n",
    "    EulerAncestralDiscreteScheduler, \n",
    "    TCDScheduler\n",
    ")\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "from transformers import CLIPVisionModelWithProjection\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(image, mask):\n",
    "    assert(image.shape[0:2]==mask.shape[0:2])\n",
    "    # canny image\n",
    "    alpha = mask / 255.0\n",
    "    # Apply the mask\n",
    "    image_on_mask = (alpha * image).astype(np.uint8)    \n",
    "    return image_on_mask\n",
    "\n",
    "def canny_edge_detection(img, low_threshold=50, high_threshold=200, apertureSize=3, L2gradient=True):\n",
    "    # Apply Canny edge detection\n",
    "    image_canny = cv2.Canny(img, low_threshold, high_threshold, apertureSize=apertureSize, L2gradient=L2gradient)\n",
    "    # Convert the single-channel image to a three-channel image by stacking\n",
    "    image_canny = np.stack((image_canny,)*3, axis=-1)\n",
    "    return image_canny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling of the generation the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"device:\", device)\n",
    "\n",
    "model_id_or_path = \"SG161222/RealVisXL_V5.0_Lightning\" #\"stabilityai/stable-diffusion-xl-base-1.0\" #\n",
    "vae_id_or_path = \"madebyollin/sdxl-vae-fp16-fix\"\n",
    "\n",
    "controlnet_canny = ControlNetModel.from_pretrained(\"xinsir/controlnet-canny-sdxl-1.0\", weight_name=\"diffusion_pytorch_model_V2.safetensors\", torch_dtype=torch.float16)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(vae_id_or_path, torch_dtype=torch.float16)\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "    \"h94/IP-Adapter\",\n",
    "    subfolder=\"models/image_encoder\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "pipe_tex2img = AutoPipelineForText2Image.from_pretrained(model_id_or_path,\n",
    "                                                         controlnet=controlnet_canny,\n",
    "                                                         vae=vae,\n",
    "                                                         use_safetensors=True,\n",
    "                                                         torch_dtype=torch.float16,\n",
    "                                                         image_encoder=image_encoder,\n",
    "                                                       )              \n",
    "\n",
    "# Setting up of others parameters\n",
    "pipe_tex2img.enable_xformers_memory_efficient_attention()\n",
    "pipe_tex2img = pipe_tex2img.to(device)\n",
    "\n",
    "# Update of the scheduler\n",
    "pipe_tex2img.scheduler = TCDScheduler.from_config(pipe_tex2img.scheduler.config)\n",
    "\n",
    "# Add IP-Adapter to the model\n",
    "pipe_tex2img.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter-plus_sdxl_vit-h.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and repositionong of image and mask #src/mea_genai/img/\n",
    "image_input = load_image(\"img/image_6.png\")\n",
    "mask_image = load_image(\"img/image_6_mask.png\") \n",
    "\n",
    "image_input_array = np.array(image_input)\n",
    "mask_image_array = np.array(mask_image)\n",
    "\n",
    "# Apply the mask\n",
    "input_image_on_mask = apply_mask(image_input_array, mask_image_array)\n",
    "image_canny = Image.fromarray(canny_edge_detection(input_image_on_mask, low_threshold=25, high_threshold=65, apertureSize=3, L2gradient=False))\n",
    "\n",
    "# Load your images using load_image\n",
    "images = [\n",
    "    image_input,\n",
    "    mask_image,\n",
    "    image_canny,\n",
    "]\n",
    "make_image_grid(images, rows=1, cols=len(images), resize=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image with nothing\n",
    "prompt_per_ip_adapter_index = [(12, \"The image shows a living room with a chair and a potted plant on the left side, a table with cups and saucers in the middle, and a carpet on the floor. On the wall behind the chair, there are three framed prints of a mountain scene, adding a touch of nature to the room\"),\n",
    "                               (13, \"The image shows a living room with a grey couch sitting on top of a wooden floor, a carpet, a table with a lamp and other objects, a pot with a plant, a wall with a photo frame, and a glass window with trees visible outside.\"),\n",
    "                               (14, \"The image shows a brown leather accent chair in a living room with a white rug on the floor, a cupboard with books and a photo frame on top, a window with a curtain, and a wall in the background.\")]\n",
    "seed_per_ip_adapter_index = [(12, 33588),\n",
    " (14, 127871),\n",
    " (13, 366081),\n",
    " (12, 416179),\n",
    " (14, 466573),\n",
    " (13, 910052)]\n",
    "\n",
    "# Number of images per batch\n",
    "N = 2\n",
    "\n",
    "# Loop over both lists\n",
    "for index, (ip_adapter_index, prompt) in enumerate(prompt_per_ip_adapter_index):\n",
    "    # Find the corresponding seed for this ip_adapter_index\n",
    "    seeds_for_current_adapter = [seed for (seed_ip_adapter_index, seed) in seed_per_ip_adapter_index if seed_ip_adapter_index == ip_adapter_index]\n",
    "\n",
    "    ip_adapter_image = load_image(f\"img/inspiration/inspiration_{ip_adapter_index}.png\") \n",
    "    \n",
    "    # Iterate through the seeds for the current ip_adapter_index\n",
    "    for seed in seeds_for_current_adapter:\n",
    "        # Set the seed\n",
    "        generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "        print(f\"seed: {seed}\")\n",
    "\n",
    "        # Set ip-adapter scaling\n",
    "        pipe_tex2img.set_ip_adapter_scale(0.8)\n",
    "\n",
    "        # Set control net scaling\n",
    "        controlnet_conditioning_scale = 0.6\n",
    "\n",
    "        # Print the prompt for reference\n",
    "        print(\"prompt: \", prompt)\n",
    "\n",
    "        # Run the inference\n",
    "        images_output = pipe_tex2img(\n",
    "            prompt=prompt,\n",
    "            image=image_canny,\n",
    "            ip_adapter_image=ip_adapter_image,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            num_inference_steps=10,\n",
    "            num_images_per_prompt=N,\n",
    "            guidance_scale=1.7,\n",
    "            generator=generator\n",
    "        ).images\n",
    "\n",
    "        # Make image grid and save images\n",
    "        make_image_grid(images_output, rows=1, cols=N, resize=512)\n",
    "        \n",
    "        # Save each image with unique filename\n",
    "        for i, image_output in enumerate(images_output):\n",
    "            plt.imsave(f'ai_image_sdxl_seed_{seed}_{index}_ip_adapter_{ip_adapter_index}_{i}.png', np.array(image_output).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over both lists\n",
    "for index, (ip_adapter_index, prompt) in enumerate(prompt_per_ip_adapter_index):\n",
    "    # Find the corresponding seed for this ip_adapter_index\n",
    "    seeds_for_current_adapter = [seed for (seed_ip_adapter_index, seed) in seed_per_ip_adapter_index if seed_ip_adapter_index == ip_adapter_index]\n",
    "\n",
    "    ip_adapter_image = load_image(f\"img/inspiration/inspiration_{ip_adapter_index}.png\") \n",
    "    \n",
    "    # Iterate through the seeds for the current ip_adapter_index\n",
    "    for seed in seeds_for_current_adapter:\n",
    "        # Set the seed\n",
    "        generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "        print(f\"seed: {seed}\")\n",
    "\n",
    "        # Set ip-adapter scaling\n",
    "        pipe_tex2img.set_ip_adapter_scale(0)\n",
    "\n",
    "        # Set control net scaling\n",
    "        controlnet_conditioning_scale = 0.6\n",
    "\n",
    "        # Print the prompt for reference\n",
    "        print(\"prompt: \", prompt)\n",
    "\n",
    "        # Run the inference\n",
    "        images_output = pipe_tex2img(\n",
    "            prompt=prompt,\n",
    "            image=image_canny,\n",
    "            ip_adapter_image=ip_adapter_image,\n",
    "            controlnet_conditioning_scale=controlnet_conditioning_scale,\n",
    "            num_inference_steps=10,\n",
    "            num_images_per_prompt=N,\n",
    "            guidance_scale=1.7,\n",
    "            generator=generator\n",
    "        ).images\n",
    "\n",
    "        # Make image grid and save images\n",
    "        make_image_grid(images_output, rows=1, cols=N, resize=512)\n",
    "        \n",
    "        # Save each image with unique filename\n",
    "        for i, image_output in enumerate(images_output):\n",
    "            plt.imsave(f'ai_image_sdxl_seed_{seed}_{index}_vanilla_{i}.png', np.array(image_output).astype(np.uint8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nfinite_genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
